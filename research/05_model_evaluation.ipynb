{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af4f19f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved to: C:\\Users\\user\\anaconda3\\envs\\Chicken-disease-Classification-Project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "project_path = r\"C:\\Users\\user\\anaconda3\\envs\\Chicken-disease-Classification-Project\"\n",
    "\n",
    "os.chdir(project_path)\n",
    "print(f\"Moved to: {os.getcwd()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01c3d305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\user\\\\anaconda3\\\\envs\\\\Chicken-disease-Classification-Project'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ceef8c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4fe6b5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"artifacts/training/model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97b8ee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvaluationConfig:\n",
    "    trained_model_path: Path\n",
    "    training_data: Path\n",
    "    all_params: dict\n",
    "    params_image_size: List[int]\n",
    "    params_batch_size: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b773b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnnclassifier.constants import *\n",
    "from cnnclassifier.utils.common import read_yaml, create_directories, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52bbf6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath: str = CONFIG_FILE_PATH, params_filepath: str = PARAM_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config['artifacts_root']])\n",
    "\n",
    "    def get_validation_config(self) -> EvaluationConfig:\n",
    "        model_params = self.params.get(\"MODEL_PARAMS\", {})\n",
    "    \n",
    "        image_size = model_params.get(\"IMAGE_SIZE\")\n",
    "        batch_size = model_params.get(\"BATCH_SIZE\")\n",
    "    \n",
    "        if image_size is None or batch_size is None:\n",
    "            raise KeyError(\"IMAGE_SIZE or BATCH_SIZE not found in MODEL_PARAMS section of params.yaml\")\n",
    "    \n",
    "        eval_config = EvaluationConfig(\n",
    "            trained_model_path=os.path.join(\"artifacts\", \"training\", \"model.keras\"),\n",
    "            training_data=os.path.join(\"artifacts\", \"data_ingestion\", \"Chicken-fecal-images\"),\n",
    "            all_params=self.params,\n",
    "            params_image_size=image_size,\n",
    "            params_batch_size=batch_size\n",
    "        )\n",
    "        return eval_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3ce1935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bd526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from pathlib import Path\n",
    "\n",
    "class Evaluation:\n",
    "    def __init__(self, config: EvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def _load_model(self):\n",
    "        self.model = tf.keras.models.load_model(self.config.trained_model_path)\n",
    "\n",
    "    def _load_validation_data(self):\n",
    "        datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            rescale=1.0 / 255,\n",
    "            validation_split=0.2  # same as training\n",
    "        )\n",
    "\n",
    "        self.val_generator = datagen.flow_from_directory(\n",
    "            directory=self.config.training_data,\n",
    "            target_size=self.config.params_image_size[:2],\n",
    "            batch_size=self.config.params_batch_size,\n",
    "            class_mode=\"categorical\",\n",
    "            subset=\"validation\",\n",
    "            shuffle=False  # important for label alignment\n",
    "        )\n",
    "\n",
    "    def _evaluate_model(self):\n",
    "        self._load_model()\n",
    "        self._load_validation_data()\n",
    "\n",
    "        self.score = self.model.evaluate(self.val_generator)\n",
    "        self.y_pred = self.model.predict(self.val_generator)\n",
    "        self.y_pred_classes = np.argmax(self.y_pred, axis=1)\n",
    "        self.y_true = self.val_generator.classes\n",
    "\n",
    "        self.classification_report = classification_report(\n",
    "            self.y_true, self.y_pred_classes, output_dict=True\n",
    "        )\n",
    "        self.conf_matrix = confusion_matrix(self.y_true, self.y_pred_classes)\n",
    "\n",
    "    def save_score(self):\n",
    "        scores = {\n",
    "            \"loss\": self.score[0],\n",
    "            \"accuracy\": self.score[1],\n",
    "            \"classification_report\": self.classification_report,\n",
    "            \"confusion_matrix\": self.conf_matrix.tolist()\n",
    "        }\n",
    "\n",
    "        save_path = Path(\"artifacts/evaluation/scores.json\")\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)  # Create directory if missing\n",
    "\n",
    "        save_json(path=save_path, data=scores)\n",
    "\n",
    "    def evaluate(self):\n",
    "        self._evaluate_model()\n",
    "        self.save_score()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e743d5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-05 16:14:39,754: INFO: 2793454304: Initializing ConfigurationManager...]\n",
      "[2025-06-05 16:14:39,762: INFO: common: YAML file: config\\config.yaml loaded successfully]\n",
      "[2025-06-05 16:14:39,773: INFO: common: YAML file: params.yaml loaded successfully]\n",
      "[2025-06-05 16:14:39,776: INFO: common: Created directory at: artifacts]\n",
      "[2025-06-05 16:14:39,778: INFO: 2793454304: ConfigurationManager initialized.]\n",
      "[2025-06-05 16:14:39,778: INFO: 2793454304: Fetching evaluation configuration...]\n",
      "[2025-06-05 16:14:39,780: INFO: 2793454304: Evaluation configuration fetched.]\n",
      "[2025-06-05 16:14:39,783: INFO: 2793454304: Initializing Evaluation class...]\n",
      "[2025-06-05 16:14:39,783: INFO: 2793454304: Evaluation class initialized.]\n",
      "[2025-06-05 16:14:39,783: INFO: 2793454304: Starting model evaluation...]\n",
      "Found 78 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\chicken\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4s/step - accuracy: 0.9672 - loss: 0.1409\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 4s/step\n",
      "[2025-06-05 16:15:25,187: INFO: common: JSON file saved at: artifacts\\evaluation\\scores.json]\n",
      "[2025-06-05 16:15:25,194: INFO: 2793454304: Model evaluation completed successfully.]\n",
      "[2025-06-05 16:15:25,197: INFO: 2793454304: Saving evaluation scores...]\n",
      "[2025-06-05 16:15:25,201: INFO: common: JSON file saved at: artifacts\\evaluation\\scores.json]\n",
      "[2025-06-05 16:15:25,204: INFO: 2793454304: Evaluation scores saved successfully.]\n"
     ]
    }
   ],
   "source": [
    "from cnnclassifier import logger  # your custom logger\n",
    "\n",
    "try:\n",
    "    logger.info(\"Initializing ConfigurationManager...\")\n",
    "    config = ConfigurationManager()\n",
    "    logger.info(\"ConfigurationManager initialized.\")\n",
    "\n",
    "    logger.info(\"Fetching evaluation configuration...\")\n",
    "    val_config = config.get_validation_config()  # call the method\n",
    "    logger.info(\"Evaluation configuration fetched.\")\n",
    "\n",
    "    logger.info(\"Initializing Evaluation class...\")\n",
    "    evaluation = Evaluation(val_config)\n",
    "    logger.info(\"Evaluation class initialized.\")\n",
    "\n",
    "    logger.info(\"Starting model evaluation...\")\n",
    "    evaluation.evaluate()  # call your evaluation method\n",
    "    logger.info(\"Model evaluation completed successfully.\")\n",
    "\n",
    "    logger.info(\"Saving evaluation scores...\")\n",
    "    evaluation.save_score()  # call method to save scores\n",
    "    logger.info(\"Evaluation scores saved successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.exception(\"An error occurred during the evaluation pipeline execution.\")\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f741063a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chicken",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
